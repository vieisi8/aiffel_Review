# 학습 내용

---

- Masked Autoencoders Are Scalable Vision Learners

---

## Masked Autoencoders Are Scalable Vision Learners

---

---

### Abstract

---

  - masked autoencoder(MAE)가 컴퓨터 비전을 위한 확장 가능한  self-supervised learner(자기 지도 학습자) 임을 보여줌
  - 입력 이미지의 무작위 패치를 마스킹하고 누락된 픽셀을 재구성하는 MAE 접근 방식은 간단함
  - 두가지 핵심 설계를 기반으로 함
    - 비대칭 인코더-디코더 아키텍쳐를 개발해
      - 마스크 토큰 없이 보이는 패치 하위 집합에서만 작동하는 인코더
      - 잠재 표현 및 마스크 토큰으로부터 원본 이미지를 재구성하는 경량 디코더
    -  입력 이미지의 높은 비율(ex) 75%)을 마스킹하면 사소하지 않고 의미 있는 자체 감독 작업을 수행할 수 있다는 사실 발견
  - 위 두가지 기법을 결합하면
    - 대규모 모델을 효율적이고 효과적으로 훈련할 수 있음
    - 훈련 속도(3배 이상)와 정확도를 높일 수 있음
  - 확장 가능한 접근 방식을 통해 일반화가 잘 되는 고용량 모델을 학습할 수 있음
  - 다운 스트림 작업에서의 전송 성능은 감독된 사전 훈련보다 뛰어나며 유망한 확장 동작을 보여줌

---

### 1. Introduction

---

![image](https://github.com/user-attachments/assets/279a38aa-3250-47e4-9541-375be4a96ca6)

  - 인코더
    - 보이는 패치의 작은 하위 집합에 적용
    - 마스크 토큰은 인코더 다음에 도입
  - 디코더
    - 인코딩된 패치와 마스크 토큰의 전체 세트는 원본 이미지를 픽셀 단위로 재구성하는 소형 디코더에 의해 처리 됨
  - 사전 학습 후 인코더만 사용하며, 인식 작업을 위해 손상되지 않은 이미지(전체 패치 세트)에 적용


---

### 3. Approach

---

