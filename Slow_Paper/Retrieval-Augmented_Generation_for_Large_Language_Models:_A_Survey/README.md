# 학습 내용

---

## Retrieval-Augmented Generation for Large Language Models: A Survey

---

Abstract

  - LLM은 인상적인 기능을 제공하지만
    - hallucination(환각), 오래된 지식, 투명하지 않고 추적할 수 없는 추론 과정과 같은 문제에 직면함
  - 검색 증강 생성(RAG)
    - 외부 데이터베이스의 지식을 통합해 유망한 솔루션으로 떠오름
    - 지식 집약적인 작업에서 생성의 정확성과 신뢰성 향상
    - 지속적인 지식 업데이트와 도메인별 정보의 통합을 가능하게 함
    - LLM 내부 지식 + 외부 데이터베이스 => 시너지 효과 발휘
  - Naive RAG, Advanced RAG, Modular RAG 를 아우르는 RAG 패러다임의 발전에 대해 살펴볼 예정
    - 검색, 생성, 증강 기술을 포함하는 RAG 프레임워크
    - RAG 시스템의 발전에 심도 있는 이해 제공
  - 최신 평가 프레임워크와 벤치마크 소개
  - 현재 직면한 과제 설명, 향후 연구 및 개발 방향 제시

1. INTRODUCTION

  - LLM은 주목할 만한 성공을 거두었지만
    - 도메인별 또는 지식 집약적 작업에서 상당한 한계에 직면해 있음
    - 학습 데이터 이상의 쿼리를 처리하거나 최신 정보가 필요할때 hallucination(환각) 발생
  - 위와 같은 문제를 극복하기 위한 검색 증강 생성(RAG)
    - 의미적 유사성 계산을 통해 외부 지식 기반에서 관련 문서 덩어리를 검색함으로써 LLM을 향상시킴
    - 외부 지식을 참조함으로써 사실과 다른 콘텐츠를 생성하는 문제를 효과적으로 줄임
    - 실제 애플리케이션에 대한 LLM의 적합성을 향상시키는 핵심 기술로 자리 잡음

![image](https://github.com/user-attachments/assets/6a47e8cd-7a4e-4a66-9da5-3a319b3c0ec1)
  
  - RAG의 발전 과정
    1. RAG의 시작
       - 트랜스포머 아키텍처의 부상과 동시에 이루어짐
       - 사전 훈련 모델(PTM)을 통해 추가 지식을 통합해 언어 모델을 향상시키는 데 중점을 둠
       - 사전 학습 기법을 개선하기 위한 기초 작업이 특징
    2. 중요한 전환점
       - ChatGPT가 등장하면서 LLM이 in context learning (ICL) 기능을 보여줌
       - RAG 연구는 추론 단계에서 더 나은 정보를 제공하는 방향으로 전환
       - 연구가 진행됨에 따라 더 이상 추론 단계에만 국한되지 않고 LLM 미세 조정 기술에 더 많은 것을 통합하기 시작
  - 체계적인 통합
    - 100개가 넘는 RAG 연구의 세 가지 주요 연구 패러다임을 요약
    - 검색, 생성, 증강의 핵심 단계의 주요 기술 분석
  - 평가 방법
    - 기존 연구는 방법론에 더 집중하는 경향이 있어 RAG를 평가하는 방법에 대한 분석과 요약이 부족함
    - RAG에 적용할 수 있는 다운스트림 작업, 데이터셋, 벤치마크, 평가 방법을 종합적으로 검토
  - 기초적인 기술 개념, 역사적 발전 과정, LLM 이루 등장한 RAG 방법론과 애플리케이션의 스펙트럼 정리, 분류
  - 대규모 모델과 RAG에 대한 상세하고 체계적인 이해를 갖출 수 있도록 설계
  - 기여
    - Naive RAG, Advanced RAG, Modular RAG 를 포함한 패러다임의 진화 설명
    - 검색, 생성, 증강 측면에 초점을 맞춰 RAG 프로세스에 필수적인 핵심 기술 식별, 논의
    - 어떻게 복잡하게 협업하여 응집력 있고 효과적인 RAG 프레임워크를 형성하는지 설명, 시너지 효과 살펴보기
    - RAG의 현재 평가 방법 요약
    - 현재의 과제를 해결하기 위한 잠재적인 개선 사항 강조
    - RAG의 향후 방향 예상
   
2. OVERVIEW OF RAG

![image](https://github.com/user-attachments/assets/d4f1f3e9-6fbb-4eb9-9bfa-41502ed52d44)

  - 위 그림은 RAG의 일반적인 적용 사례
    - 외부 데이터베이스에서 지식을 소싱하고 통합해 정보 격차 해소
    - 외부 데이터베이스와 원래의 질문과 결합되어 종합적인 프롬프트를 형생해 LLM이 충분한 정보를 바탕으로 답변을 생성할 수 있도록 지원

![image](https://github.com/user-attachments/assets/269a918f-4fa7-4d44-8d3f-b75abb9d42e5)

  - RAG 연구 패러다임을 세 단계로 분류
    - RAG 방식은 비용 효율적이고 네이티브 LLM의 성능을 능가
    - 몇가지 한계가 존재 -> Naive RAG의 단점을 보안하기 위해 Advanced RAG, Modular RAG 개발됨
    1. Naive RAG
       - ChatGPT이 채택한 직후 주목을 받은 초기 방법론
       - Indexing(인덱싱), Retrieval(검색), Generation(생성)을 포함하는 전통적인 프로세스를 따름
       - Retrieve-Read 프레임워크
       - Indexing
          1. PDF, HTML, Word, Markdown과 같은 다양한 형식의 원시 데이터 정리, 추출하는 것으로 시작
          2. 이를 균일한 일반 텍스트 형식으로 변환
          3. 언어 모델의 컨텍스트 제한을 수용하기 위해 텍스트는 더 작고 소화 가능한 청크로 분할
          4. 청크는 임베딩 모델을 사용해 벡터 표현으로 인코딩되어 벡터 DB에 저장됨
          - Retrieval 단계에서 효율적인 유사도 검색을 가능하게 하는데 매우 중요함
       - Retrieval
          1. 사용자 쿼리를 수신하면 RAG 시스템은 인코딩 모델을 사용해 쿼리를 벡터 표현으로 변환
          2. 쿼리간의 유사성 계산
              - 벡터와 인덱싱된 말뭉치 내의 청크 벡터 사용
          3. 쿼리와 가장 유사성이 높은 상위 K 청크의 우선 순위 정해 검색
          4. 이 청크는 이후 프롬프트에서 확장된 문맥으로 사용됨
       - Generation
          1. 제시된 쿼리와 선택된 문서는 일관된 프롬프트로 합성되 LLM이 응답을 공식화하는 작업을 수행
               - 고유한 파라메트릭 지식을 활용하거나 제공된 문서에 포함된 정보로 답변을 제한할 수 있음
       - 눈에 띄는 단점 존재
         - Retrieval Challenges
           - Retrieval 단계에서 종종 정확도와 회상에 어려움을 겪어 정렬이 잘못 되거나 관련 없는 청크 선택
           - 중요한 정보가 누락
         - Generation Difficulties
           - 검색된 컨텍스트에 의해 뒷바침되지 않은 결론을 생성하는 hallucination(환각) 문제에 직면할 수 있음
           - 결과의 관련성, 독성 또는 편향성으로 인해 응답의 품질과 신뢰성이 저하될 수 있음
         - Augmentation Hurdles
           - 검색된 정보를 다른 작업과 통합하는 것은 어려울 수 있음
           - 때로는 일관성 없는 결과물이 나올 수 있음
           - 여러 소스에서 유사한 정보를 검색할 때 프로세스에서 중복이 발생해 반복적인 응답 발생할 수 있음
           - 복잡한 문제에 직면했을 때, 원래 쿼리를 기반르로 한 단일 검색만으로는 적절한 맥락 정보를 얻기에 충분하지 않을 수 있음
           - 증강 정보에 지나치게 의존해 인사이트를 추가하지 않고, 검색된 콘텐츠를 괴풀이하는 결과물이 나올 수 있다는 우려 존재
    2. Advanced RAG
       - Naive RAG의 한계를 극복하기 위해 구체적인 개선 사항 도입
         - 재검증 품질 향상에 중점을 두고 사전 검색 및 사후 검색 전략 사용
       - Indexing 문제를 해결하기 위해
         - 슬라이딩 원도우 접근 방식, fine-grained 세분화, 메타데이터의 통합 도입
       - Retrieval 프로세스를 간소화하기 위해 몇가지 최적화 방법을 통합
         - Pre-retrieval process
           - Indexing 구조와 원본 쿼리를 최적화 하는데 중점을 둠
           - Indexing 최적화 목표
             - 인덱싱되는 콘텐츠의 품질 향상
             - 데이터 세분성 향상, Indexing 구조 회적화, 메타데이터 추가, 정렬 최적화, 혼합 검색 등의 전략 포함
           - 쿼리 최적화 목표
             - 사용자의 원래 질문을 더 명확하고 Retrieval 작업에 더 적합하게 만드는 것
             - 일반적인 방법으로는 쿼리 재작성, 쿼리 변환, 쿼리 확장 및 기타 기법 존재
         - Post-Retrieval Process
           - 관련성 있는 문맥이 검색되면 이를 쿼리와 효과적으로 통합하는 것이 중요
           - 주요 방법에는 청크 재순위 지정, 컨텍스트 압축이 포함됨
           - 핵심 전략
             - 검색된 정보의 순위를 재조정해 가장 관련성이 높은 콘텐츠를 프롬프트의 가장자리로 재배치하는 것
             - 이 개념은 LlamaIndex, LangChain, HayStack와 같은 프레임워크에 구현됨
             - 처리할 문맥을 줄이는 데 집중
    3. Modular RAG
       - 기존의 두가지 RAG 패러다임을 뛰어넘는 향상된 적응성과 다용도성을 제공
       - 유사도 검색을 위한 검색 모듈 추가, 미세 조정을 통해 Retrieval를 개선하는 등 구성 요소를 개선하기 위한 다양한 전략 통합
       - 재구조화 된 RAG 모듈, 재배치된 RAG 파이프라인과 같은 혁신 도입
       - 앤드투엔드 훈련 지원
       - Advanced RAG 및 Naive RAG의 기본 원칙을 기반으로 구축됨
         1. New Modules
            - 검색 및 처리 기능을 향상시키기 위해 추가적인 전문 구성 요소 도입
            - 특정 시나리오에 맞게 조정되어 LLM에서 생성된 코드와 쿼리 언어를 사용해 검색 엔진, 데이터베이스, 지식 그래프롸 같은 다양한 데이터 소스에서 직접 검색 할 수 있게 해줌
            - RAG-Fusion
              - 사용자 쿼리를 다양한 관점으로 확장하고 병렬 벡터 검색과 지능형 재순위를 활용하여 명시적 지식과 변형적 지식을 모두 발견하는 다중 쿼리 전략을 채택
              - 기존의 Retrieval 한계를 해결
            - 메모리 모듈
              - LLM의 메모리를 활용하여 검색을 안내해 반복적인 자체 향상을 통해 텍스트를 데이터 분포와 더 가깝게 정렬하는 무제한 메모리 풀 생성
            - RAG 시스템의 라우팅
              - 다양한 데이터 소스를 탐색해 요약, 특정 데이터베이스 검색 또는 서로 다른 정보 스트림 병합 등
                - 쿼리에 대한 최적의 경로 선택
            - 예측 모듈
              - LLM을 통해 직접 컨텍스트를 생성해 관련성솨 장확성를 보장함
              - 중복과 노이즈를 줄이는 것을 목표로 함
            - 작업 어댑터 모듈
              - RAG를 다양한 다운스트림 작업에 맞게 조정해 zero-shot 입력에 대한 신속한 검색을 자동화
              - few-shot 쿼리 생성을 통해 작업별 검색기를 생성
              - Retrieval 프로세스를 간소화할 뿐만 아니라 검색된 정보의 품질과 관련성을 크게 개선해
                - 다양한 작업 및 향상된 정밀도와 유연성으로 쿼리에 대응
         2. New Patterns
            - 놀라운 적응력 제공
              - 모듈을 대체하거나 재구성해 특정 문제를 해결할 수 있는 기능 제공
              - Advanced RAG 및 Naive RAG의 고정된 구조를 뛰어넘는 것
              - 새로운 모듈을 통합하거나 기존 모듈 간의 상호 작용 흐름을 조정함으로써 이러한 유연성을 확정해 다양한 작업에 걸쳐 적용 가능성을 높임
            - Rewrite-Retrieve-Read
              - 재작성 모듈과 재작성 모델을 업데트하는 LM 피드백 메커니즘을 통해
                - 검색 쿼리를 구체화하는 LLM의 기능을 활용해 작업 성능 개선
            - Generate-Read
              - 전통적인 Retrieval을 LLM에서 생성된 콘텐츠로 대체
            - ReciteRead
              - 모델 가중치에서 검색을 강조해 지식 집약적인 작업을 처리하는 모델의 능력 향상
            - 하이브리드 검색 전략
              - 키워드, 시맨틱, 벡터 검색을 통합해 다양한 쿼리에 대응
            - 하위 쿼리와 가상 문서 임베딩(HyDE) 사용
              - 사용된 답변과 실제 문서 간의 유사성을 임베딩하는 데 집중해 검색 관련성을 개선할 수 있음
       - RAG 시스템을 다른 기술보다 쉽게 통합할 수 있다는 것
         - 더 나은 검색 결과를 위해 Retrieval를 미세 조정하거나, 보다 개인화된 결과물을 위해 Generation를 미세조정할 수 있음

![image](https://github.com/user-attachments/assets/d58db1f3-f841-46fb-97fa-5b173447b39f)

  - RAG vs Fine-tuning
    - RAG는 종종 미세 조정, 프롬프트 엔지니어링과 비교되기도 함
      - 위 그림에 표시된 것 처럼 각 방법에는 고유한 특성 존재
    - RAG
      - 가전 정보 검색 작업에 이상적
      - 실시간 지식 업데이트와 해석 가능성이 높은 외부 지식 소스의 효과적인 활용을 제공함으로써 역동적인 환경에서 탁월한 성능 발휘
      - 지연 시간이 길고 데이터 검색과 관련해 윤리적 고려 사항 존재
    - 미세 조정
      - 정적이어서 업데이트를 위해 재교육이 필요함
      - 모델의 동작과 스타일을 심층적으로 사용자 지정할 수 있음
      - 데이터셋 준비 및 훈련에 상당한 컴퓨팅 리소스 필요
      - hallucination(환각)을 줄일 수 있지만 익숙하지 않은 데이터로 인해 어려움을 겪을 수 있음
    - 훈련 중에 접한 기존 지식과 완전히 새로운 지식 모두에서 RAG가 일관되게 더 나은 성과를 보임
    - RAG와 미세 조정은 상호 베타적이지 않으며 서로를 보완하여 다양한 수준에서 모델의 기능을 향상시킬 수 있음

3.RETRIEVAL

  - 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이 중요
  1. Retrieval Source
     - LLM을 향상시키기 위해 외부 지식에 의존하며, 검색 소스의 유형과 검색 단위의 세분성은 최종 생성 결과에 영향을 미침
     1. Data Structure
        - 초기에는 텍스트가 검색 소스의 주류였음
        - 그 후 반정혈 데이터(PDF)와 정형 데이터(지식 그래프, KG)로 확장
        - 검색 및 고도화 목적으로 LLM이 자체적으로 생성한 콘텐츠를 활용하는 연구도 증가하는 추세
        - 비정형 데이터
          - 가장 널리 사용되는 검색 소스
          - 주로 말뭉치에서 수집
          - ODQA 작업의 경우, 주요 버전 HotpotQA, DPR
            - 백과사전 외에도 일반적인 비정형 데이터에는 언어 간 텍스트와 도메인별 데이터가 포함
        - 반정형 데이터
          - 일반적으로 PDF와 같이 텍스트와 표 정보가 결합된 데이터
          - 크게 두 가지 이유로 인해 기존 RAG 시스템에 문제를 야기함
            1. 텍스트 분할 프로세스가 실수로 테이블을 분리하여 검색 중에 데이터가 손상될 수 있음
            2. 데이터에 테이블을 통합하면 의미적 유사성 검색이 복잡해질 수 있음
          - 접근 방식
            - TableGPT와 같이 데이터베이스 내의 테이블에 대해 Text-2-SQL 쿼리를 실행하기 위해 LLM의 기능을 활용하는 것
            - 텍스트 기반 방법을 사용해 추가 분석을 위해 테이블을 텍스트 형식으로 변환
            - 위 두가지 방법 모두 최적의 솔루션은 아니므로 이 분야에 대한 후속 연구가 필요함
        - 정형 데이터
          - 일반적으로 검증되고 보다 정확한 인포메이션을 제공
          - KnowledGPT
            - KB 검색 쿼리를 생성하고 개인화된 기반에 지식을 저장해 RAG 모델의 지식 풍부도를 향상
          - 텍스트 그래프에 대한 질문을 이해하고 답변하는 데 있어 LLM의 한계에 대응하는 G-Retriver
              - Graph Neural Networks(GNN), LLM 및 RAG를 통합하여 LLM의 소프트 프롬프트를 통해 그래프 이해 및 질문 응답 기능 향상
              - 타겟 그래프 검색을 위해 Prize-Collecting Steiner Tree(PCST) 최적화 문제를 사용
          - 구조화된 데이터베이스를 구축, 검증 및 유지 관리하는 데 추가적인 노력이 필요
        - LLM이 생성한 콘텐츠
          - RAG에서 외부 보조 정보의 한계를 해결하기 위해 일부 연구는 LLM의 내부 지식을 활용하는데 중점을 둠
          - SKR
            - 질문을 알려지거나 알려지지 않은 것으로 분류해 검색 향상을 선택적으로 적용
          - GenRead
            - Retriver를 LLM 생성기로 대체함
            - LLM이 생성한 문맥이 더 정확한 답변을 포함하는 경우가 많다는 사실 발견
          - Selfmem
            - 검색 강화 생성기로 무제한 메모리 풀을 반복적으로 생성
            - 메모리 선택기를 사용해 원래 질문에 대한 이중 문제로 작용하는 출력을 선택
            - 생성 모델을 자체적으로 강화
          - 이러한 방법론은 RAG의 혁신적인 데이터 소스 활용 폭을 강조
     2. Retrieval Granularity
        - 또 다른 중요한 요소는 검색된 데이터의 세분성
        - 조잡한 검색 단위
          - 이론적으로 문제에 대해 더 많은 관련 정보를 제공할 수 있음
          - 중복 콘텐츠를 포함할 수 있어 다운 스트임 작업에서 검색기와 언어 모델의 주의를 분산시킬 수 있음
        - 검색 단위의 세분화
          - 검샏의 부담을 증가시키고, 의미 무결성과 필요한 지식 충족을 보장하지 못함
          - 적절한 검색 세분성을 선택하는것
            - 밀도가 높은 retriever의 검색 및 다운스트림 작업 성능을 향상시키는 간단하고 효과적인 전략이 될 수 있음
          - DenseX
            - 명제를 검색 단위로 사용하는 개념을 제안
            - 명제
              - 텍스트의 원자 표현으로 정의
              - 각각 고유한 사실 세그먼트를 캡슐화하여 간결하고 독립적인 자연어 형식으로 제시
            - 검색 정확도와 관련성을 향상시키는 것을 목표로 함
          - 지식 그래프에서 검색 세분성
            - 엔티티, 삼중 항 및 하위 그래프가 포함됨
          - 추천 작업에서 항목 ID를 검색하거나 문장 쌍을 검색하는 등의 다운스트림 작업에도 적용될 수 있음, 밑에 표1 참조
![image](https://github.com/user-attachments/assets/87820de4-8a7f-401c-8f9d-da5f0e1e28a4)
  2. Indexing Optimization
     - Indexing 단계에서는 문서를 처리하고 세그먼트화해 임베딩으로 변환하여 벡터 DB에 저장
     - Indexing 구성의 품질에 따라 검색 단계에서 올바른 컨텍스트를 얻을 수 있는지 여부 결정
     1. Chunking Strategy
        - 가장 일반적인 방법은 문서를 고정된 토큰수로 분할하는 것
        - 청크가 클때
          - 더 많은 컨텍스트를 캡처할 수 있음
          - 더 많은 노이즈가 발생해 처리 시간이 길어지고 비용이 더 많이 듬
        - 청크가 작을때
          - 필요한 컨텍스트를 완전히 전달하지 못할 수도 있음
          - 노이즈가 적음
          - 문장 내에서 잘림을 유발해 재귀적 분할 및 슬라이딩 윈도우 방식의 최적화를 유도
          - 여러 검색 프로세스에 걸쳐 전 서계적으로 관련된 정보를 변합해 계층화된검색을 가능하게 함
        - 여전히 의미적 완정성과 문맥 길이 사이의 균형을 맟출 수 없음
        - Small2Big
          - 문장(작은)을 검색 단위로 사용, 앞뒤의 문장을(큰) 컨텍스트로 LLM에 제공하는 방법
     2. Metadata Attachments
        - 청크는 페이지 번호, 파일 이름, 작성자, 카테고리 타임스탬프와 같은 메타데이터 정보로 보강할 수 있음
        - 이후 메타데이터 기반으로 검색을 필터링해 검색 범위를 제한할 수 있음
        - 문서 타임스탬프에 서로 다른 가중치를 할당
          - 시간 인식 RAG를 구현해 지식의 최신성을 보장하고 오래된 정보를 피할 수 있음
        - 메타데이터를 인위적으로 구성할 수도 있음
          - 리버스 HyDE 방법이라고도 함
          - 구체적으로는 LLM을 사용해 문서에서 답변할 수 있는 질문을 생성
          - 검색 중에 원래 질문과 가상 질문 간의 유사성을 계산해 질문과 답변 사이의 의미적 차이를 줄이는 것
     3. Structural Index
        - 정보 검색을 향상시키는 효과적인 방법 중 하나
          - 문서에 계층 구조를 설정하는 것
          - 관련 데이터의 검색과 처리를 신속하게 처리할 수 있음
         1. Hierarchical index structure
            - 파일
              - 부모와 자식 관계로 배열되며, 청크가 연결되어 있음
            - 데이터 합계
              - 각 노드에 저장되어 데이터의 신속한 탐색에 도움을 줌
              - RAG 시스템이 추출할 청크를 결정하는데 도움을 줌
            - 블록 추출 문제로 인한 hallucination(환각) 현상을 완화할 수 있음
         2. Knowledge Graph index
            - 문서의 계층을 구성하는 데 KG를 활용하면 일관성을 유지하는데 기여
            - 서로 다른 개념과 엔티티 간의 연결을 묘사해 착각의 가능성을 현저히 줄여줌
            - 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환해 지식 검색의 전확성을 높이고 LLM이 맥략적으로 일관된 응답을 생성할 수 있게 함으로써 RAG 시스템의 전반적인 효율성 향상
            - KGP
              - 문서 내용과 구조 간의 논리적 관계를 포착하기 위해 KG를 사용해 여러 문서 간에 인덱스를 구축하는 방법 제안
              - 노드(페이지나 표와 같은 문서 내 단락이나 구조를 타나내는),에지(단락 간의 의미적/어휘적 유사성 또는 문서 구조 내 관계를 나타내는)로 구성해 다중 문서 환경에서 지식 에지 검색 및 추론 문제를 효과적으로 해결
