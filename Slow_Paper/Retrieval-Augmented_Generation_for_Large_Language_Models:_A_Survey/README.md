# 학습 내용

---

## Retrieval-Augmented Generation for Large Language Models: A Survey

---

### Abstract

---

  - LLM은 인상적인 기능을 제공하지만
    - hallucination(환각), 오래된 지식, 투명하지 않고 추적할 수 없는 추론 과정과 같은 문제에 직면함
  - 검색 증강 생성(RAG)
    - 외부 데이터베이스의 지식을 통합해 유망한 솔루션으로 떠오름
    - 지식 집약적인 작업에서 생성의 정확성과 신뢰성 향상
    - 지속적인 지식 업데이트와 도메인별 정보의 통합을 가능하게 함
    - LLM 내부 지식 + 외부 데이터베이스 => 시너지 효과 발휘
  - Naive RAG, Advanced RAG, Modular RAG 를 아우르는 RAG 패러다임의 발전에 대해 살펴볼 예정
    - 검색, 생성, 증강 기술을 포함하는 RAG 프레임워크
    - RAG 시스템의 발전에 심도 있는 이해 제공
  - 최신 평가 프레임워크와 벤치마크 소개
  - 현재 직면한 과제 설명, 향후 연구 및 개발 방향 제시

---

### 1. INTRODUCTION

---

  - LLM은 주목할 만한 성공을 거두었지만
    - 도메인별 또는 지식 집약적 작업에서 상당한 한계에 직면해 있음
    - 학습 데이터 이상의 쿼리를 처리하거나 최신 정보가 필요할때 hallucination(환각) 발생
  - 위와 같은 문제를 극복하기 위한 검색 증강 생성(RAG)
    - 의미적 유사성 계산을 통해 외부 지식 기반에서 관련 문서 덩어리를 검색함으로써 LLM을 향상시킴
    - 외부 지식을 참조함으로써 사실과 다른 콘텐츠를 생성하는 문제를 효과적으로 줄임
    - 실제 애플리케이션에 대한 LLM의 적합성을 향상시키는 핵심 기술로 자리 잡음

![image](https://github.com/user-attachments/assets/6a47e8cd-7a4e-4a66-9da5-3a319b3c0ec1)
  
  - RAG의 발전 과정
    1. RAG의 시작
       - 트랜스포머 아키텍처의 부상과 동시에 이루어짐
       - 사전 훈련 모델(PTM)을 통해 추가 지식을 통합해 언어 모델을 향상시키는 데 중점을 둠
       - 사전 학습 기법을 개선하기 위한 기초 작업이 특징
    2. 중요한 전환점
       - ChatGPT가 등장하면서 LLM이 in context learning (ICL) 기능을 보여줌
       - RAG 연구는 추론 단계에서 더 나은 정보를 제공하는 방향으로 전환
       - 연구가 진행됨에 따라 더 이상 추론 단계에만 국한되지 않고 LLM 미세 조정 기술에 더 많은 것을 통합하기 시작
  - 체계적인 통합
    - 100개가 넘는 RAG 연구의 세 가지 주요 연구 패러다임을 요약
    - 검색, 생성, 증강의 핵심 단계의 주요 기술 분석
  - 평가 방법
    - 기존 연구는 방법론에 더 집중하는 경향이 있어 RAG를 평가하는 방법에 대한 분석과 요약이 부족함
    - RAG에 적용할 수 있는 다운스트림 작업, 데이터셋, 벤치마크, 평가 방법을 종합적으로 검토
  - 기초적인 기술 개념, 역사적 발전 과정, LLM 이루 등장한 RAG 방법론과 애플리케이션의 스펙트럼 정리, 분류
  - 대규모 모델과 RAG에 대한 상세하고 체계적인 이해를 갖출 수 있도록 설계
  - 기여
    - Naive RAG, Advanced RAG, Modular RAG 를 포함한 패러다임의 진화 설명
    - 검색, 생성, 증강 측면에 초점을 맞춰 RAG 프로세스에 필수적인 핵심 기술 식별, 논의
    - 어떻게 복잡하게 협업하여 응집력 있고 효과적인 RAG 프레임워크를 형성하는지 설명, 시너지 효과 살펴보기
    - RAG의 현재 평가 방법 요약
    - 현재의 과제를 해결하기 위한 잠재적인 개선 사항 강조
    - RAG의 향후 방향 예상
   
---

### 2. OVERVIEW OF RAG

---

![image](https://github.com/user-attachments/assets/d4f1f3e9-6fbb-4eb9-9bfa-41502ed52d44)

  - 위 그림은 RAG의 일반적인 적용 사례
    - 외부 데이터베이스에서 지식을 소싱하고 통합해 정보 격차 해소
    - 외부 데이터베이스와 원래의 질문과 결합되어 종합적인 프롬프트를 형생해 LLM이 충분한 정보를 바탕으로 답변을 생성할 수 있도록 지원

![image](https://github.com/user-attachments/assets/269a918f-4fa7-4d44-8d3f-b75abb9d42e5)

  - RAG 연구 패러다임을 세 단계로 분류
    - RAG 방식은 비용 효율적이고 네이티브 LLM의 성능을 능가
    - 몇가지 한계가 존재 -> Naive RAG의 단점을 보안하기 위해 Advanced RAG, Modular RAG 개발됨
    1. Naive RAG
       - ChatGPT이 채택한 직후 주목을 받은 초기 방법론
       - Indexing(인덱싱), Retrieval(검색), Generation(생성)을 포함하는 전통적인 프로세스를 따름
       - Retrieve-Read 프레임워크
       - Indexing
          1. PDF, HTML, Word, Markdown과 같은 다양한 형식의 원시 데이터 정리, 추출하는 것으로 시작
          2. 이를 균일한 일반 텍스트 형식으로 변환
          3. 언어 모델의 컨텍스트 제한을 수용하기 위해 텍스트는 더 작고 소화 가능한 청크로 분할
          4. 청크는 임베딩 모델을 사용해 벡터 표현으로 인코딩되어 벡터 DB에 저장됨
          - Retrieval 단계에서 효율적인 유사도 검색을 가능하게 하는데 매우 중요함
       - Retrieval
          1. 사용자 쿼리를 수신하면 RAG 시스템은 인코딩 모델을 사용해 쿼리를 벡터 표현으로 변환
          2. 쿼리간의 유사성 계산
              - 벡터와 인덱싱된 말뭉치 내의 청크 벡터 사용
          3. 쿼리와 가장 유사성이 높은 상위 K 청크의 우선 순위 정해 검색
          4. 이 청크는 이후 프롬프트에서 확장된 문맥으로 사용됨
       - Generation
          1. 제시된 쿼리와 선택된 문서는 일관된 프롬프트로 합성되 LLM이 응답을 공식화하는 작업을 수행
               - 고유한 파라메트릭 지식을 활용하거나 제공된 문서에 포함된 정보로 답변을 제한할 수 있음
       - 눈에 띄는 단점 존재
         - Retrieval Challenges
           - Retrieval 단계에서 종종 정확도와 회상에 어려움을 겪어 정렬이 잘못 되거나 관련 없는 청크 선택
           - 중요한 정보가 누락
         - Generation Difficulties
           - 검색된 컨텍스트에 의해 뒷바침되지 않은 결론을 생성하는 hallucination(환각) 문제에 직면할 수 있음
           - 결과의 관련성, 독성 또는 편향성으로 인해 응답의 품질과 신뢰성이 저하될 수 있음
         - Augmentation Hurdles
           - 검색된 정보를 다른 작업과 통합하는 것은 어려울 수 있음
           - 때로는 일관성 없는 결과물이 나올 수 있음
           - 여러 소스에서 유사한 정보를 검색할 때 프로세스에서 중복이 발생해 반복적인 응답 발생할 수 있음
           - 복잡한 문제에 직면했을 때, 원래 쿼리를 기반르로 한 단일 검색만으로는 적절한 맥락 정보를 얻기에 충분하지 않을 수 있음
           - 증강 정보에 지나치게 의존해 인사이트를 추가하지 않고, 검색된 콘텐츠를 괴풀이하는 결과물이 나올 수 있다는 우려 존재
    2. Advanced RAG
       - Naive RAG의 한계를 극복하기 위해 구체적인 개선 사항 도입
         - 재검증 품질 향상에 중점을 두고 사전 검색 및 사후 검색 전략 사용
       - Indexing 문제를 해결하기 위해
         - 슬라이딩 원도우 접근 방식, fine-grained 세분화, 메타데이터의 통합 도입
       - Retrieval 프로세스를 간소화하기 위해 몇가지 최적화 방법을 통합
         - Pre-retrieval process
           - Indexing 구조와 원본 쿼리를 최적화 하는데 중점을 둠
           - Indexing 최적화 목표
             - 인덱싱되는 콘텐츠의 품질 향상
             - 데이터 세분성 향상, Indexing 구조 최적화, 메타데이터 추가, 정렬 최적화, 혼합 검색 등의 전략 포함
           - 쿼리 최적화 목표
             - 사용자의 원래 질문을 더 명확하고 Retrieval 작업에 더 적합하게 만드는 것
             - 일반적인 방법으로는 쿼리 재작성, 쿼리 변환, 쿼리 확장 및 기타 기법 존재
         - Post-Retrieval Process
           - 관련성 있는 문맥이 검색되면 이를 쿼리와 효과적으로 통합하는 것이 중요
           - 주요 방법에는 청크 재순위 지정, 컨텍스트 압축이 포함됨
           - 핵심 전략
             - 검색된 정보의 순위를 재조정해 가장 관련성이 높은 콘텐츠를 프롬프트의 가장자리로 재배치하는 것
             - 이 개념은 LlamaIndex, LangChain, HayStack와 같은 프레임워크에 구현됨
             - 처리할 문맥을 줄이는 데 집중
    3. Modular RAG
       - 기존의 두가지 RAG 패러다임을 뛰어넘는 향상된 적응성과 다용도성을 제공
       - 유사도 검색을 위한 검색 모듈 추가, 미세 조정을 통해 Retrieval를 개선하는 등 구성 요소를 개선하기 위한 다양한 전략 통합
       - 재구조화 된 RAG 모듈, 재배치된 RAG 파이프라인과 같은 혁신 도입
       - 앤드투엔드 훈련 지원
       - Advanced RAG 및 Naive RAG의 기본 원칙을 기반으로 구축됨
         1. New Modules
            - 검색 및 처리 기능을 향상시키기 위해 추가적인 전문 구성 요소 도입
            - 특정 시나리오에 맞게 조정되어 LLM에서 생성된 코드와 쿼리 언어를 사용해 검색 엔진, 데이터베이스, 지식 그래프와 같은 다양한 데이터 소스에서 직접 검색 할 수 있게 해줌
            - RAG-Fusion
              - 사용자 쿼리를 다양한 관점으로 확장하고 병렬 벡터 검색과 지능형 재순위를 활용하여 명시적 지식과 변형적 지식을 모두 발견하는 다중 쿼리 전략을 채택
              - 기존의 Retrieval 한계를 해결
            - 메모리 모듈
              - LLM의 메모리를 활용하여 검색을 안내해 반복적인 자체 향상을 통해 텍스트를 데이터 분포와 더 가깝게 정렬하는 무제한 메모리 풀 생성
            - RAG 시스템의 라우팅
              - 다양한 데이터 소스를 탐색해 요약, 특정 데이터베이스 검색 또는 서로 다른 정보 스트림 병합 등
                - 쿼리에 대한 최적의 경로 선택
            - 예측 모듈
              - LLM을 통해 직접 컨텍스트를 생성해 관련성과 장확성를 보장함
              - 중복과 노이즈를 줄이는 것을 목표로 함
            - 작업 어댑터 모듈
              - RAG를 다양한 다운스트림 작업에 맞게 조정해 zero-shot 입력에 대한 신속한 검색을 자동화
              - few-shot 쿼리 생성을 통해 작업별 검색기를 생성
              - Retrieval 프로세스를 간소화할 뿐만 아니라 검색된 정보의 품질과 관련성을 크게 개선해
                - 다양한 작업 및 향상된 정밀도와 유연성으로 쿼리에 대응
         2. New Patterns
            - 놀라운 적응력 제공
              - 모듈을 대체하거나 재구성해 특정 문제를 해결할 수 있는 기능 제공
              - Advanced RAG 및 Naive RAG의 고정된 구조를 뛰어넘는 것
              - 새로운 모듈을 통합하거나 기존 모듈 간의 상호 작용 흐름을 조정함으로써 이러한 유연성을 확정해 다양한 작업에 걸쳐 적용 가능성을 높임
            - Rewrite-Retrieve-Read
              - 재작성 모듈과 재작성 모델을 업데트하는 LM 피드백 메커니즘을 통해
                - 검색 쿼리를 구체화하는 LLM의 기능을 활용해 작업 성능 개선
            - Generate-Read
              - 전통적인 Retrieval을 LLM에서 생성된 콘텐츠로 대체
            - ReciteRead
              - 모델 가중치에서 검색을 강조해 지식 집약적인 작업을 처리하는 모델의 능력 향상
            - 하이브리드 검색 전략
              - 키워드, 시맨틱, 벡터 검색을 통합해 다양한 쿼리에 대응
            - 하위 쿼리와 가상 문서 임베딩(HyDE) 사용
              - 사용된 답변과 실제 문서 간의 유사성을 임베딩하는 데 집중해 검색 관련성을 개선할 수 있음
       - RAG 시스템을 다른 기술보다 쉽게 통합할 수 있다는 것
         - 더 나은 검색 결과를 위해 Retrieval를 미세 조정하거나, 보다 개인화된 결과물을 위해 Generater를 미세조정할 수 있음

![image](https://github.com/user-attachments/assets/d58db1f3-f841-46fb-97fa-5b173447b39f)

  - RAG vs Fine-tuning
    - RAG는 종종 미세 조정, 프롬프트 엔지니어링과 비교되기도 함
      - 위 그림에 표시된 것 처럼 각 방법에는 고유한 특성 존재
    - RAG
      - 사전 정보 검색 작업에 이상적
      - 실시간 지식 업데이트와 해석 가능성이 높은 외부 지식 소스의 효과적인 활용을 제공함으로써 역동적인 환경에서 탁월한 성능 발휘
      - 지연 시간이 길고 데이터 검색과 관련해 윤리적 고려 사항 존재
    - 미세 조정
      - 정적이어서 업데이트를 위해 재교육이 필요함
      - 모델의 동작과 스타일을 심층적으로 사용자 지정할 수 있음
      - 데이터셋 준비 및 훈련에 상당한 컴퓨팅 리소스 필요
      - hallucination(환각)을 줄일 수 있지만 익숙하지 않은 데이터로 인해 어려움을 겪을 수 있음
    - 훈련 중에 접한 기존 지식과 완전히 새로운 지식 모두에서 RAG가 일관되게 더 나은 성과를 보임
    - RAG와 미세 조정은 상호 베타적이지 않으며 서로를 보완하여 다양한 수준에서 모델의 기능을 향상시킬 수 있음

---

### 3.RETRIEVAL

---

  - 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이 중요
  1. Retrieval Source
     - LLM을 향상시키기 위해 외부 지식에 의존하며, 검색 소스의 유형과 검색 단위의 세분성은 최종 생성 결과에 영향을 미침
     1. Data Structure
        - 초기에는 텍스트가 검색 소스의 주류였음
        - 그 후 반정형 데이터(PDF)와 정형 데이터(지식 그래프, KG)로 확장
        - 검색 및 고도화 목적으로 LLM이 자체적으로 생성한 콘텐츠를 활용하는 연구도 증가하는 추세
        - 비정형 데이터
          - 가장 널리 사용되는 검색 소스
          - 주로 말뭉치에서 수집
          - ODQA 작업의 경우, 주요 버전 HotpotQA, DPR
            - 백과사전 외에도 일반적인 비정형 데이터에는 언어 간 텍스트와 도메인별 데이터가 포함
        - 반정형 데이터
          - 일반적으로 PDF와 같이 텍스트와 표 정보가 결합된 데이터
          - 크게 두 가지 이유로 인해 기존 RAG 시스템에 문제를 야기함
            1. 텍스트 분할 프로세스가 실수로 테이블을 분리하여 검색 중에 데이터가 손상될 수 있음
            2. 데이터에 테이블을 통합하면 의미적 유사성 검색이 복잡해질 수 있음
          - 접근 방식
            - TableGPT와 같이 데이터베이스 내의 테이블에 대해 Text-2-SQL 쿼리를 실행하기 위해 LLM의 기능을 활용하는 것
            - 텍스트 기반 방법을 사용해 추가 분석을 위해 테이블을 텍스트 형식으로 변환
            - 위 두가지 방법 모두 최적의 솔루션은 아니므로 이 분야에 대한 후속 연구가 필요함
        - 정형 데이터
          - 일반적으로 검증되고 보다 정확한 인포메이션을 제공
          - KnowledGPT
            - KB 검색 쿼리를 생성하고 개인화된 기반에 지식을 저장해 RAG 모델의 지식 풍부도를 향상
          - 텍스트 그래프에 대한 질문을 이해하고 답변하는 데 있어 LLM의 한계에 대응하는 G-Retriver
              - Graph Neural Networks(GNN), LLM 및 RAG를 통합하여 LLM의 소프트 프롬프트를 통해 그래프 이해 및 질문 응답 기능 향상
              - 타겟 그래프 검색을 위해 Prize-Collecting Steiner Tree(PCST) 최적화 문제를 사용
          - 구조화된 데이터베이스를 구축, 검증 및 유지 관리하는 데 추가적인 노력이 필요
        - LLM이 생성한 콘텐츠
          - RAG에서 외부 보조 정보의 한계를 해결하기 위해 일부 연구는 LLM의 내부 지식을 활용하는데 중점을 둠
          - SKR
            - 질문을 알려지거나 알려지지 않은 것으로 분류해 검색 향상을 선택적으로 적용
          - GenRead
            - Retriver를 LLM 생성기로 대체함
            - LLM이 생성한 문맥이 더 정확한 답변을 포함하는 경우가 많다는 사실 발견
          - Selfmem
            - 검색 강화 생성기로 무제한 메모리 풀을 반복적으로 생성
            - 메모리 선택기를 사용해 원래 질문에 대한 이중 문제로 작용하는 출력을 선택
            - 생성 모델을 자체적으로 강화
          - 이러한 방법론은 RAG의 혁신적인 데이터 소스 활용 폭을 강조
     2. Retrieval Granularity
        - 또 다른 중요한 요소는 검색된 데이터의 세분성
        - 조잡한 검색 단위
          - 이론적으로 문제에 대해 더 많은 관련 정보를 제공할 수 있음
          - 중복 콘텐츠를 포함할 수 있어 다운 스트임 작업에서 검색기와 언어 모델의 주의를 분산시킬 수 있음
        - 검색 단위의 세분화
          - 검색의 부담을 증가시키고, 의미 무결성과 필요한 지식 충족을 보장하지 못함
          - 적절한 검색 세분성을 선택하는것
            - 밀도가 높은 retriever의 검색 및 다운스트림 작업 성능을 향상시키는 간단하고 효과적인 전략이 될 수 있음
          - DenseX
            - 명제를 검색 단위로 사용하는 개념을 제안
            - 명제
              - 텍스트의 원자 표현으로 정의
              - 각각 고유한 사실 세그먼트를 캡슐화하여 간결하고 독립적인 자연어 형식으로 제시
            - 검색 정확도와 관련성을 향상시키는 것을 목표로 함
          - 지식 그래프에서 검색 세분성
            - 엔티티, 삼중 항 및 하위 그래프가 포함됨
          - 추천 작업에서 항목 ID를 검색하거나 문장 쌍을 검색하는 등의 다운스트림 작업에도 적용될 수 있음, 밑에 표1 참조
![image](https://github.com/user-attachments/assets/87820de4-8a7f-401c-8f9d-da5f0e1e28a4)
  2. Indexing Optimization
     - Indexing 단계에서는 문서를 처리하고 세그먼트화해 임베딩으로 변환하여 벡터 DB에 저장
     - Indexing 구성의 품질에 따라 검색 단계에서 올바른 컨텍스트를 얻을 수 있는지 여부 결정
     1. Chunking Strategy
        - 가장 일반적인 방법은 문서를 고정된 토큰수로 분할하는 것
        - 청크가 클때
          - 더 많은 컨텍스트를 캡처할 수 있음
          - 더 많은 노이즈가 발생해 처리 시간이 길어지고 비용이 더 많이 듬
        - 청크가 작을때
          - 필요한 컨텍스트를 완전히 전달하지 못할 수도 있음
          - 노이즈가 적음
          - 문장 내에서 잘림을 유발해 재귀적 분할 및 슬라이딩 윈도우 방식의 최적화를 유도
          - 여러 검색 프로세스에 걸쳐 전 서계적으로 관련된 정보를 변합해 계층화된검색을 가능하게 함
        - 여전히 의미적 완정성과 문맥 길이 사이의 균형을 맟출 수 없음
        - Small2Big
          - 문장(작은)을 검색 단위로 사용, 앞뒤의 문장을(큰) 컨텍스트로 LLM에 제공하는 방법
     2. Metadata Attachments
        - 청크는 페이지 번호, 파일 이름, 작성자, 카테고리 타임스탬프와 같은 메타데이터 정보로 보강할 수 있음
        - 이후 메타데이터 기반으로 검색을 필터링해 검색 범위를 제한할 수 있음
        - 문서 타임스탬프에 서로 다른 가중치를 할당
          - 시간 인식 RAG를 구현해 지식의 최신성을 보장하고 오래된 정보를 피할 수 있음
        - 메타데이터를 인위적으로 구성할 수도 있음
          - 리버스 HyDE 방법이라고도 함
          - 구체적으로는 LLM을 사용해 문서에서 답변할 수 있는 질문을 생성
          - 검색 중에 원래 질문과 가상 질문 간의 유사성을 계산해 질문과 답변 사이의 의미적 차이를 줄이는 것
     3. Structural Index
        - 정보 검색을 향상시키는 효과적인 방법 중 하나
          - 문서에 계층 구조를 설정하는 것
          - 관련 데이터의 검색과 처리를 신속하게 처리할 수 있음
         1. Hierarchical index structure
            - 파일
              - 부모와 자식 관계로 배열되며, 청크가 연결되어 있음
            - 데이터 합계
              - 각 노드에 저장되어 데이터의 신속한 탐색에 도움을 줌
              - RAG 시스템이 추출할 청크를 결정하는데 도움을 줌
            - 블록 추출 문제로 인한 hallucination(환각) 현상을 완화할 수 있음
         2. Knowledge Graph index
            - 문서의 계층을 구성하는 데 KG를 활용하면 일관성을 유지하는데 기여
            - 서로 다른 개념과 엔티티 간의 연결을 묘사해 착각의 가능성을 현저히 줄여줌
            - 정보 검색 프로세스를 LLM이 이해할 수 있는 지침으로 변환해 지식 검색의 전확성을 높이고 LLM이 맥략적으로 일관된 응답을 생성할 수 있게 함으로써 RAG 시스템의 전반적인 효율성 향상
            - KGP
              - 문서 내용과 구조 간의 논리적 관계를 포착하기 위해 KG를 사용해 여러 문서 간에 인덱스를 구축하는 방법 제안
              - 노드(페이지나 표와 같은 문서 내 단락이나 구조를 타나내는),에지(단락 간의 의미적/어휘적 유사성 또는 문서 구조 내 관계를 나타내는)로 구성해 다중 문서 환경에서 지식 에지 검색 및 추론 문제를 효과적으로 해결
  3. Query Optimization
     - 나이브 RAG의 주요 문제점 중 하나
       - 사용자의 원래 쿼리가 검색에 직접적으로 의존
         - 정확하고 명확한 질문 구성하기 어려움
    	   - 질문 자체가 복잡하고 언어가 잘 정리되어 있지 않음
    	   - 언어의 복잡성 모호성
      1. 쿼리 확장
         - 단일 쿼리를 여래 개의 쿼리로 확장
           - 생성된 답변의 관련성을 최적으로 보장
         - 다중 쿼리
           - 프롬프트 엔지니어링을 사용한 LLM을 통해 쿼리를 확장해 쿼리를 병렬로 실행 가능
         - 하위 질문
           - 원래 질문을 결합했을 때 문맥을 파악하고 완전한 답변을 제공하는 데 필요한 하위 질문 생성
         - 검증 체인
           - 확장된 쿼리는 LLM의 검증을 거쳐 hallucinations을 줄이는 효과
      2. 쿼리 변환
         - 사용자의 원래 쿼리 대신 변환된 쿼리를 기반으로 청크 검색
         - 쿼리 재작성
             1. LLM에 쿼리를 다시 작성하라는 메시지 표시할 수 있음
			          - RRR과 같은 특수한 소규모 언어 모델을 사용할 수 있음
			          - 타오바오에서 구현한 BEQUE는 긴 쿼리에 대한 리콜 효과 향상, GMV 증가
		         2. 프롬프트 엔지니어링을 사용한 LLM이 원본 쿼리를 기반으로 쿼리를 생성
			          - 스텝백 프롬프트 방법을 사용해 원래 쿼리를 추상화해 높은 수준의 개념 질문을 생성
      3. 쿼리 라우팅
         - 다용도  RAG 시스템에 적합한 별도의  RAG 파이프라인으로 라우팅
         - 메타데이터 라우팅/필터
           - 키워드를 추출한 다음 키워드와 메타데이터를 기반으로 필터링해 검색
         - 시멘틱 라우터
           - 쿼리의 시맨틱 정보를 활용하는 또 다른 라우팅 방법
  4. 임베딩
     - RAG에서 Retrieval은 쿼리 임베딩과 문서 청크 간의 유사도를 계산해 이루어짐
     - 임베딩 모델의 의미 표현 기능이 핵심
	   - 스파스 인코더와 고밀도 Retrieval가 포함
	   - 어떤 임베딩 모델을 사용해야 하는가에 대한 정답 없음
	   - 특정 사용 사례에는 특정 모델이 더 적합함
       1. 혼합/하이브리드 검색
          - 스파스 및 고밀도 임베딩 접근 방식은 서로 다른 연관성 특징 포착, 상호 보완적 연관성 정보를 활용해 서로 이점을 활용 가능
       2. 임베딩 모델 미세 조정
          - 문맥이 사전 학습 코퍼스와 크게 벗아나는 경우, 자체 도메인 데이터셋에서 임베딩 모델 미세 조정이 필수
          - Retrieval와 generator를 연동하는 것
            - 프롬프터는 LLM을 숏샷 쿼리 생성기로 사용해 작업별 Retrieval을 생성하고, 특히 데이터가 부족한 도메인에서 감독 미세 조정의 문제를 해결
          - 여러 다운 스트림 작업에 걸쳐 보상 신호를 생성하기 위해 LLM 활용
            - Retrieval는 데이터 세트에 대한 하드 레이블과 LLM의 소프트 보상 두가지 유형의 지고 신호로 미세 조정
            - REOLUG
              - Retrieval와 LLM을 사용해 검색된 문서의 확률 분포를 계산한 다음 KL divergence 계산해 지도 학습 수행
            - RLHF에서 영감을 얻음
  5. 어댑터
     - 모델을 미세조정하는 데는 여려 어려움이 존재
     - 외부 어탭터를 통합해 조정에 도움을 주는 방법을 선택하기도 함
     - LLM의 멀티 태스크 기능을 최적화 하기 위한 방법들
       - up-rise
         - 미리 구축된 프롬프트 풀에서 주어진 제로샷 작업 입력에 적합한 프롬프트를 자동으로 검색할 수 있는 경량 프롬프트 Retrieval 훈련함
       - AAR(증강 적응형 Retrieval)
         - 여러 다운 스트림 작업을 수용하도록 설계된 범용 어탭터 도입
       - PRCA
         - 플러그형 보상 기반 컨텍스트 어댑터 추가
       - BGM
         - 리트리버롸 LLM을 고정, 브리지 seq2seq모델 훈련
         - 브리지 모델은 검색된 정보를 LLM이 효과적으로 작업할 수 있는 형식으로 변환, 순위 재조정, 고급 전략을 사용할 수 있도록 하는 것을 목표
       - PKG
         - 지시어 미세 조정을 통해 지식을 화이트 박스 모델에 통합하는 혁신적인 방법 도입
         - Retrieval모듈을 직접 대체해 쿼리에 따라 관련 문서 생성

---

### 4. GENERATION

---

  - 검색후 검색된 모든 정보를 LLM에 직접 입력하는 것은 좋은 방법이 아님
    1. 컨텍스트 큐레이션
       - 중복된 정보는 LLM의 최종 생성을 방해할 수있음
       - 지나치게 긴 문맥은 LLM이 중간에 길을 잃는 문제을 일으킬 수도 있음
       - 따라서 RAG 시스템에서는 일반적으로 검색된 콘텐츠를 추가로 처리 해랴함
         1. 재링크
            - 문서 청크의 순서를 재조정해 가장 관련성이 높은 결과를 먼저 강조
            - 다양성, 연관성, MRR과 같은 사전 정의된 메트릭에 의존하는  규칙 기반이나 RERT, 특수 재랭크  모델, GPT 같은 일반적인 대규모 언어 모델을 사용하는 모델 기반 접근 방식을 통해 수행
         2. 컨텍스트 선택/압축
            - RAG 프로세스에서 흔히 오해하는 것
              - 가능한 많은 관련 문서를 검색, 이를 연결해 긴 검색 프롬프트를 구성하는것이 유리하다는 믿음
            - 과도한 컨텍스트는 더 많은 노이즈를 유발해 핵심 정보에 대한 LLM의 인식을 저하
            - 작은 모델은 중요하지 않은 토크을 감지, 제거해 LLM이 잘 이해할 수 있는 형태로 변환
              - 즉각적인 압축을 위해 직접적이고 실용적인 방법
              - 언어 무결성과 압축률의 균형을 유지, LLM에 대한 추가 교육 X
              - PRCA
                - 정보 추출기를 훈련시켜 이 문제 해결
              - RECOMP
                - 대조 학습을 사용해 정보 압축기를 훈련
            - 문서 수를 줄이는 것도 정확도를 높이는데 도움이 됨
              - LLM과, SLM의 강점을 결합한 필터랭커 패러다임을 제안
              - SLM 필터 역할, LLM은 재정렬 레리전트 역할
            - 최종 답변을 생성하기 전에 검색된 콘텐츠를  LLM이 평가하도록 하는것
              - LLM 비평을 통해 관련성이 낮은 문서를 걸러낼 수 있음
    2. LLM 미세 조정
       - 목표에 맞게 미세 조정하면 더 나은 결과를 얻을 수 있음
       - 미세 조정의 또다른 이점
         - 모델의 입력과 출력을 조정할 수 있다는점
         - 구조화된 데이터를 다루는 검색 작업의 경우
           - SANTA 프레임워크
             - 구조적 및 의미론적 뉘앙스를 모두 효과적으로 캡슐화 하기 위해 삼자 훈련 요법 구현
             - 대조 학습을 활용해 Retrieval에 중점을 둠
       - 강화학습을 통해 사람 또는 Retrieval의 선호도에 맞게 결과를 조정하는것도 잠재적인 접근 방식
         - 최종 생성된 답변에 수동으로 주석을 단 다음 강화 학습을 통해 피드백 제공
       - 강력한 모델이나 더 큰 매캐변수를 가진 오픈 소스에 액세스할 수 없다면
         - 더 강력한 모델을 증류시킴
       - LLM의 미세조정을 Retrieval의 미세조정과 연계해 선호도를 조정 가능
         - RA-DIT
           - KL divergence을 사용해 리트리버와 생성기 간의 점수 함수를 연결

---

### 5. AUGMENTATION PROCESS IN RAG

---

  - RAG 영역에서 표준 관행은 비효율성을 초래할 수 있으며 제한된 범위의 정보를 제공하기 때문에 다단계 추론이 필요한  복잡한 문제에는 적합하지 않은 경우가 많음
    1. 반복 검색
       - 초기 쿼리와 지금까지 생성된 텍스트를 기반으로 지식 창고를 반복적으로 검색해 LLM을 위한 보다 포괄적인 지식을 제공하는 프로세스
       - 후속 답변 생성의 견고성을 향상시키는 것으로 나타남
       - 의미론적 단절과 관련 없는 정보의 축척에 영향을 받을 수 있음
       - 특정 정보의 재샌산이 필요한 작업에 대해 검색 강화 생성과 생성 강화 검색을 활용해 시너지 효과를 내는 접근 방식 사용
       - 후속 반복에서 향상된 응답을 생성할 수 있도록 함
    2. 재귀적 검색
       - 이전 검색에서 얻은 결과를 바탕으로 검색 쿼리를 반복적으로 구체화하는 작업이 포함됨
       - 피드백 루프를 통해 가장 관련성이 높은 정보로 수렴해 검색 환경을 개선하는 것을 목표
       - IRCoT
         - 검색 프로세스를 안내하기위해 사고의 사슬을 사용
         - 획득한 결과로 CoT를 개선
       - ToC
         - 쿼리에서 모호한 부분을 체계적으로 최적화하는 명확화 트리 생성
       - 사용자의 요구 사항에 대한 지속적인 학습과 적응이 가능해 검색 결과에 대한 만족도가 향상
       - 재귀 검색과 멀티홉 검색 기술을 함께 활용하기도 함
         1. 재귀 검색
            1. 계층적 방식으로 데이터를 처리, 검색하는 구조화된 index가 포함되며, 이 index을 기반으로 검색을 수행하기 전에 문서 또는 긴 PDF의 섹션을 요약하는 것이 포함될 수 있음
            2. 그후, 문서 내에서 2차 검색을 통해 검색을 구체화해 프로세스의 재귀적 특성을 구현
         2. 멀티홉 검색
            - 그래프 구조의 데이터 소스를 더 깊이 파고들어 상호 연결된 정보를 추출하도록 설계
    3. 적응형 검색
       - LLM이 검색을 위한 최적의 순간과 콘텐츠를 능동적으로 결정할 수 있도록 함
       - RAG 프레임워크를 개선해 소싱 정보의 효율성과 관련성을 향상시킴
       - LLM이 능동적인 판단을 사용하는 광범위한 추세의 일부
       - 그래프-툴포머
         - 검색 프로세스를 여러 단계로 구분해 LLM이 능동적으로 검색기 사용
         - self-ask 기술을 적용해 몇번의 프롬프트를 통해 검색 쿼리를 시작하도록 유도
       - WebGPT
         - 강화 학습 프레임워크를 통합해 텍스트 생성 중에 검색 엔진을 사용해 자율적으로 GPT-3 모델을 훈련시킴
         - 외부 검색 엔진의 사용을 통해 GPT-3의 기능을 확장
       - Flare
         - 용어의 확률에서 알 수 있듯이 generator 프로세스의 신뢰도를 모니터링해 타이밍 검색을 자동화함
         - 확률이 특정 임계값 아래로 떨어지면 Retrieval 시스템을 활성화해 관련 정보를 수집
       - self-RAG
         - 모델이 자신의 산출물을 성찰할 수 있는 리플렉션 토큰을 도입
         - retrieve과 critic 두가지 토큰의 종류 존재
           - Retrieval
             - 활성화할 시기를 자율적으로 결정, 미리 정의된 임계값에 따라 프로세스르 트리거할 수도 있음
             - 검색하는 동안 생성기는 여러 단락에 걸쳐 조각 수준의 빔 검색을 수행해 가장 일관된 시퀀스를 도출
           - critic 점수
             - 세분화 점수를 업데이트하는데 사용
             - 추론 중에 이러한 가중치를 유연하게 조정해 모델의 동작에 맞게 조정
         - 추가 분류기나 자연어 추론 모델에 의존할 필요 X
         - 검색 메커니즘을 언제 사용할지에 대한 의사 결정 프로세스를 간소화, 선별된 답변을 생성하는 모델의 자율 판단 능력을 향상

---

### 6. TASK AND EVALUATION

---

  - RAG 모델 평가는 자연어 처리 커뮤니티에서 연구의 최전선에 서게 됨
  - 평가의 주요 목표는 RAG 모델의 성능을 이해하고 최적화는 것
  - 주로 RAG의 주요 다운스트림 작업, 데이터셋 및 RAG 시스템을 평가하는 방법 소개
    1. 다운스트림 작업
       - RAG의 핵심 작업은 기존의 단일 홉/멀티 홉 QA, 객관식, 도메인별 QA, RAG에 적합한 긴 형식의 시나리로를 포함한 질문 답변(QA)
       - 정보추출(IE), 대화 생성, 코드 검색 등과 같은 여러 다운스트림 작업으로 지속적 확장
    2. 평가 대상
       - 지금까지 RAG 모델 평가는 특정 다운스트림 작업에서의 실행을 중심으로 이루어짐
       - 이러한 평가는 당면한 작업에 적합한 기존 지표를 사용
         - 질문 답변 평가
           - EM 및 F1 score에 의존
         - 사실 확인 작업
           - 정확도를 주요 지표로 삼는 경우가 많음
         - 답변 품질을 평가
           - BLUE 및 ROUGE 메트릭도 일반적으로 사용됨
         - RALLE
           - RAG 애플리케이션의 자동 평가를 위해 설계됨
         - 작업별 메트릭을 기반으로 평가를 수행함
       - RAG 모델의 고유한 특성을 평가하기 위한 연구는 현저히 부족, 주요 평가 목표는 다음과 같음
         - 검색 품질
           - 검색 컴포넌트가 제공하는 컨텍스트의 효율성을 결정하는 데 매우 중요함
           - 검색엔진, 추천 시스템 및 정보 검색 시스템 영역의 표준 메트릭을 사용해 RAG 검색 모듈의 성능 측정
           - 적중률,MRR,NDCG와 같은 메트릭이 이러한 목적으로 일반적으로 활용됨
         - 생성 품질
           - 검색된 문맥에서 일관되고 관련성 있는 답변을 종합하는 생성자의 역량을 중심으로 이루어짐
           - 콘텐츠의 목적에 따라 라벨링되지 않은 콘텐츠와 라벨링된 콘텐츠로 분류 가능
             - 라벨이 없는 콘텐츠의 경우
               - 생성된 답변의 충실성 관연성 및 유해성 여부가 평가에 포함
             - 라벨이 지정된 콘텐츠의 경우
               - 생성된 정보의 정확성에 중점을 둠
         - 모두 수동 또는 자동 평가 방법을 통해 수행됨
    3. 평가 측면
       - RAG 모델의 최신 평가 관행은 알파 크기의 세가지 기본 품질 점수, 네가지 필수 능력으로 구성
       - 두 가지 주요 목표인 검색과 생성에 대한 평가를 종합적으로 알려줌
         1. 품질 점수
            - 컨텍스트 릴리스, 답변 충실도, 답변 관련성이 포함됨
            - 정보 검색 및 생성 과정에서 다양한 관점에서 RAG 모델의 효율성을 평가
            - 컨텍스트 관련성
              - 검색된 컨텍스트의 정확성과 구체성을 평가
              - 관련성을 보장하고, 불필요한 콘텐츠와 관련된 처리 비용 최소화
            - 답변 충실도
              - 생성된 답변이 검색된 컨텍스트에 충실하게 유지되도록함
              - 일관성을 유지, 모순 방지
            - 답변 관련성
              - 생성된 답변이 제기된 질문과 직접 관련이 있어야 함
              - 핵심 질문을 효과적으로 다룰 수 있어야 함
         2. 필수 능력
            - RAG 평가에느 정응성과 효율성을 나타내는 네가지 능력 노이즈 강건성, 부정적 거부, 정보 통합 및 사실과 반대되는 강건성도 포함
            - 다양한 과제와 복잡한 시나리오에서 모델의 성능에 매우 중요하며 품질 점수에 영향을 미침
            - 노이즈 강건성
              - 질문과 관련이 있지만 하위 정보가 부족한 노이즈 문서를 인에이징하는 모델의 기능을 평가
            - 부정적 거부
              - 검색된 문서에 질문에 답하는 데 필요한 지식이 포함괴어 있지 않을 때 응답을 자제라는 모델의 분별력 평가
            - 정보 통합
              - 복잡한 질문을 해결하기 위해 여러 문서의 정보를 종합하는 모델의 숙련도 평가
            - 사실과 반대되는 견고성
              - 잠재적인 잘못된 정보에 대한 지시가 있더라도 문서 내에서 알려진 부정확성을 인지하고 무시하는 모델의 능력을 테스트
       - 문맥 관련성 및 노이즈 견고성은 검색 품질을 평가하는 데 중요
       - 답변 충싱도, 답변 관련성, 부정적 거부, 정보 통합 및 사실과 반대되는 견고성은 생성 품질을 평가하는데 중요
    4. 평가 벤치마트 및 도구
       - 평가를 용이하게 하기 위해 일련의 벤치마크 테스트와 도구가 제안됨
       - RAG 모델 성능을 측정할 뿐만 아니라 다양한 평가 측면에서 모델의 기능에 대한 이해를 높일 수 있는 정량적 지표를 제공
       - RGB, RECALL, CRUD와 같은 저명한 벤치마크
         - RAG 모델의 본질적인 능력을 평가하는 데 중점을 둠
       - RAGAS, ARES, TruLens와 같은 최점단 자동화 도구
         - LLM을 사용해 품질 점수를 판정
       - RAG 모델의 체계적 평가를 위한 강력한 프레임워크를 종합적으로 형성
