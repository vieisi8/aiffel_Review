# 학습 내용

---

- QLORA: Efficient Finetuning of Quantized LLMs

---

## QLORA: Efficient Finetuning of Quantized LLMs

---

### 1. Introduction

---

  - LLM을 미세 조정하는 것
    - 성능 개선,
    - 바람직한 동작 추가,
    - 바람직하지 않은 동작을 제거 하는데 매우 효과적인 방법
  - 그러나 매우 큰 모델을 미세 조정하는 데는 엄청난 비용이 들며, 엄청난 GPU 메모리 필요
  - 양자화 방법
    - LLM의 메모리의 공간을 줄일 수 있음
    - 이러한 기술은 추론할 때만 작동하고 훈련 중에 고장남
  - 우리는 성능 저하 없이 양자화된 4비트 모델을 미세 조정하는 것이 가능하다는 것을 처음으로 입증함
    - 새로운 고정밀 기법인 QLORA는 사전 학습된 모델을 4비트로 양자화한 다음, 양자화된 가중치를 통해 그라데이션을 역전파하여 조정되는 학습 가능한 로우랭크 어댑터 가중치 세트를 추가하는 방식
  - QLORA는 65B 파라미터 모델을 미세 조정하는 데 필요한 평균 메모리 요구량
    - 780GB이상의 GPU 메모리 -> 48GB 미만으로 줄임
    - 16비트 완전한 미세 조정을 기준
    - 이는 중요한 LLM 미세 조정의 접근성 변화
      - 단일 GPU에서 미세 조정할 수 있는 공개적으로 사용 가능한 모델 중 가진 큰 규모
  - 

---

### 3. QLORA Finetuning

---
