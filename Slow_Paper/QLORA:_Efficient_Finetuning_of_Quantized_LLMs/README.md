# 학습 내용

---

- QLORA: Efficient Finetuning of Quantized LLMs

---

## QLORA: Efficient Finetuning of Quantized LLMs

---

### 1. Introduction

---

  - LLM을 미세 조정하는 것
    - 성능 개선,
    - 바람직한 동작 추가,
    - 바람직하지 않은 동작을 제거 하는데 매우 효과적인 방법
  - 그러나 매우 큰 모델을 미세 조정하는 데는 엄청난 비용이 들며, 엄청난 GPU 메모리 필요
  - 양자화 방법
    - LLM의 메모리의 공간을 줄일 수 있음
    - 이러한 기술은 추론할 때만 작동하고 훈련 중에 고장남
  - 우리는 성능 저하 없이 양자화된 4비트 모델을 미세 조정하는 것이 가능하다는 것을 처음으로 입증함
    - 새로운 고정밀 기법인 QLORA는 사전 학습된 모델을 4비트로 양자화한 다음, 양자화된 가중치를 통해 그라데이션을 역전파하여 조정되는 학습 가능한 로우랭크 어댑터 가중치 세트를 추가하는 방식
  - QLORA는 65B 파라미터 모델을 미세 조정하는 데 필요한 평균 메모리 요구량
    - 780GB이상의 GPU 메모리 -> 48GB 미만으로 줄임
    - 16비트 완전한 미세 조정을 기준
    - 이는 중요한 LLM 미세 조정의 접근성 변화
      - 단일 GPU에서 미세 조정할 수 있는 공개적으로 사용 가능한 모델 중 가진 큰 규모

![image](https://github.com/user-attachments/assets/652cba14-aef1-4120-94a2-0c3d0a00549e)

  - QLORA를 사용해 Guanaco 모델 제품군을 훈련
    - Guanaco 13B
      - Vicuna 벤치마크에서 ChatGPT의 97.8% 성능 수준 도달
      - 단일 소비자 GPU에서 12시간 이내 훈련
    - Guanaco 65B
      - 단일 전문가용 GPU 24시간 이상 사용
      - Vicuna 벤치마크에서 ChatGPT의 99.3% 성능 수준 달성해 ChatGPT와의 격차 좁힘
    - Guanaco 7B
      - 5GB의 메모리만 필요
      - Vicuna 벤치마크에서 26GB Alpaca 모델보다 20% 포인트 이상 성능이 뛰어
  - QLORA는 성능 저하 없이 메모리 사용량을 줄이기 위해 설계된 다양한 혁신 기술을 도입
    1. 4비트 정수 및 4비트 부동 소수점보다 더 나은 경험적 결과를 제공하는 정규 분포 데이터에 대해 이론적으로 최적의 양자화 데이터 유형인 4비트 NormalFloat
    2. 양자화 상수를 양자화해 매개변수당 평균 약 0.37bits(65B 모델의 경우 3GB)를 절약하는 방법인 이중 양자화(Double Quantization)
    3. 긴 시퀀스 길이의 미니 배치를 처리할 때 발생하는 그라데이션 체크포인트 메모리 스파크를 방지하기 위해 NVIDIA 통합 메모리를 사용하는 페이징 옵티마이저(Paged Optimizers)
  - 이러한 기술을 결합해 어탭터를 포함하는 더 나은 조정된 LoRA 접근 방식으로 이전 작업에서 볼 수 있었던 거의 모든 정확도 절충을 피할 수 있음
  - 코드 베이스와 CUDA 커널을 오픈소스화해 누구나 쉽게 접근할 수 있도록 Hugging Face 트랜스포머 스택에 메서드 통합

---

### 3. QLORA Finetuning

---

  - QLORA는 4비트 NormalFloat(NF4) 양자화 및 이중 양자화하는 두 가지 기술
    - high-fidelity 4비트 미세 조정 달성
  - 페이징 옵티마이저를 도입해 그라데이션 체크포인트 중 메모리 스파이크로 인해 메모히 부족 오류가 발생하는 것을 방지
    - 대규모 모델의 경우 단일 머신에서 미세 조정을 어렵게 만들었던 기존 문제 해결
  - QLORA에는 일반적으로 4비트인 하나의 저정밀 저장 데이터 유형과 일반적으로 BFloat16인 하나읭 계산 데이터 유형 존재
    - 실제로는 QLORA 가중 텐서를 사용할 때마다 텐서를 BFloat16으로 양자화한 다음 16비트로 행렬 곱셈 수행
  - QLORA의 구성 요소에 설명한 다음 QLORA의 공식적인 정의에 대해 설명
    1. 4비트 NormalFloat 양자화
       - NormalFlaot(NF)데이터 유형은 각 양자화 구간이 입력 텐서에서 할당된 동일한 수의 값을 갖도록 하는 정보 이론적으로 최적의 데이터 유형인 사분위수 양자화를 기반으로 함
       - 사분위수 양자화는 경험적 누적 분포 함수를 통해 입력 텐서의 사분위수를 추정하는 방식으로 작동
       - 사분의수 정량화의 가장 큰 한계
         - 사분위수 추정 프로세스가 비용이 많이 든다는 점
       - 사분위수 추정에 SRAM 사분위수와 같은 빠은 사분위수 근사 알고이즘 사용
         - 데이터 유형은 종종 가장 중요한 값인 이상값에 대해 큰 양자화 오차를 가짐
       - 사전 학습된 신경망 가중치는 일반적으로 표준 편차 σ를 갖는 영중심 정규 분포
         - 데이터 유형의 범위에 정확히 맞도록 σ를 조정해 모든 가중치를 단일 고정 분포로 변활할 수 있음
         - 임의의 범위 [-1,1] 설정
         - 따라서 모두 이 범위로 정규화해야 함
       - 임의의 표준 편차 σ가 [-1,1]범위인 영평균 정규 분포에 대한 이론적으로 최적의 정보 데이터 유형은 다음과 같이 계산됨
         1. 이론적 N(0,1) 분포의 2^k+1 사분위수를 추정해 정규 분포에 대한 k비트 사분위수 양자화 데이터 유형을 얻음
         2. 이 데이터 유형을 가져와 그 값을 [-1,1]범위로 정규화
         3. 입력 가중 텐서를 절대 최대 리스케일링(absolute maximum rescaling)을 통해[-1,1]범위로 정규화해 양자화
         - 가중치 범위와 데이터 유형 범뤼가 일치하면 평소처럼 정량화할 수 있음
         - 3 단계는 가중 텐서의 표준편차를 k비트 데이터 유형의 표준 편차와 일치하도록 재조정하는 것과 같음
         - 데이터 유형 2^k 값, qi를 다음과 같이 추정
            ![image](https://github.com/user-attachments/assets/5c12c43b-d441-4660-8d21-7aa25f69aaba)
         - 여기서 QX(·)는 표준 정규 분포 N(0,1)의 분위 함수
         - 대칭 k비트 양자화의 문제점
           - 패딩 및 기타 0값 요소를 오차 없이 양자화하는데 중요한 속성인 0 을 정확하게 표현하지 못함
         - 이산 영점 0을 보장하고 k비트 데이터 유형에 2^k 비트를 모두 사용하려면
           - qi의 두 범위
             - 음수 부분
               - 2^(k-1)
             - 양수 부분
               - 2^(k-1) + 1
           - 의 분위수 qi를 추정해 비대칭 데이터 유형을 만든 다음 이 두 집합을 통합하고 두 집합에서 발생하는 두 0 중 하나 제거
         - 각 양자화 구간에서 예상되는 값의 수가 동일한 결과 데이터 유형을 k bits NormalFloat(NFK)라고 함
           - 이 데이터 유형은 정보 이론적으로 0 중심의 정규 분포 데이터에 최적인 데이터 유형이기 때문
    2. 이중 양자화
       - 추가 메모리 절약을 위해 양자화 상수를 양자화하느 프로세스인 이줒 양자화(DQ)
       - 정밀한 4비트 양자화를 위해서느 작은 블록 크기가 필요하지만, 메모리 오버헤드도 상당함
       - 구체적으로 이중 양자화는
         - 첫 번째 양자화 상수 c^(FP32)2를 두 번째 양자화에 대한 입력으로 취급
         - 양자화된 양자화 상수 c^(FP32)2와 두 번째 양자화 상수 c^(FP32)2를 산출
         - 여기서는 블록 크기가 8비트 양자화에서는 성능 저하가 관찰되지 않음
           - 두 번째 양자화의 경우
             - 256을 사용
             - 이는  Dettmers, Zettlemoyer의 결과와 일치
         - c^(FP32)2가 양수이므로 양자화 전 C2에서 평균을 빼서 값을 0을 중심으로 매칭 양자화 사용
           - 평균적으로 64인 경우
             - 이 양자화는 매개변수당 메모리 사용량을 0.5bits에서 0.127bits로 줄여 매개변수당 0.373bits를 줄임
    3. 페이징 옵티마이저
       - NVIDIA 통합 메모리 기능을 사용
         - GPU에 가끔 메모리가 부족할 때 오류 없는 GPU 처리를 위해 CPU와 GPU 간에 페이지 간 자동 전송을 수행
         - 이 기능은 CPU, RAM과 디스크 간의 일반 메모리 페이징 처럼 작동
           - 옵티마이저 상태에서 페이징 메모리를 할당
           - GPU 메모리가 부족할 때 자동으로 CPU, RAM으로 퇴거한 다음 옵티마이저 업데이트 단계에서 메모리가 필요할 때 다시 GPU 메모리에 페이징함
    - QLORA
      - 위에서 설명한 구성 요소를 사용해 양자화된 기본 모델에서 단일 LoRA 어댑터가 있는 단일 선형 레이어에 대한 QLORA를 다음과 같이 정의
        ![image](https://github.com/user-attachments/assets/2a40520d-57fc-46c9-a0eb-7c064c5359f7)
      - doubleDequant(·)은 다음과 같이 정의
        ![image](https://github.com/user-attachments/assets/6a756696-ac3b-4bfa-bf42-d43f33e6ea73)
      - W에는 NF4를, c2에는 FP8을 사용
      - 양자화 정밀도를 높이기 위해
        - W에는 64의 블록 크기 사용
        - 메모리 절약을 위해 c2에는 256의 블록 크기 사용
      - 파라미터 업데이트의 경우
        - 어댑터 가중치 ∂E/∂Li에 대한 오차에 대한 기울기만 필요
        - 4비트 가중치 ∂E/∂W에는 필요하지 않음
        - 그러나 ∂E/∂Li의 계산은 ∂X/∂W의 계산을 수반
        - 이 계산은 스토리지 WNF4에서 계산 데이터 유형WBF16으로 양자화하여 미분 ∂X/∂W를 BFloat16 정밀도로 계산하는 방정식을 통해 진행됨
      - 요약하자면
        - QLORA에는 하나의 저장 데이터 유형(보통 4비트 NormalFloat)과 계산 데이터 유형(16비트 BrainFloat)이 있음
        - 저장 데이터 유형을 계산 데이터 유형으로 양자화하여 정방향 및 역방향 패스를 수행하지만,
        - 16비트 BrainFloat를 사용하는 LoRA 매개변수에 대한 가중치 그라데이션만 계산

